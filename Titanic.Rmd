---
title: 'Kaggle: Titanic Survival'
author: "nathan kurtzenko"
date: "7/17/2019"
output:
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: console
---

##Introduction

The purpose of this Rmarkdown document is to explore different modeling techniques to predict as accurately as possible whether or not an individual on boad the Titanic was to survive. We will produce a few different models. It would be most useful to use classification models rather than regressive models considering the predicted output. However, we will attempt a linear regressive model as well in our efforts. 

##Import Libraries

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(FNN)
library(ggplot2)
library(tibble)
library(stats)
library(DAAG)
library(randomForest)
library(boot)
library(rfUtilities)
```

##Import Train and Test Data

```{r}
train.df <- read.csv("C:\\Users\\Marvin\\Desktop\\Nathan Coding Stuff\\Data Sets\\Titanic\\train.csv")

test.df <- read.csv("C:\\Users\\Marvin\\Desktop\\Nathan Coding Stuff\\Data Sets\\Titanic\\test.csv")
```

##Data Cleaning

###Search for NA Values

```{r}
numvars <- ncol(train.df)
numColNA.df <- tibble(colnum = 1:numvars, NAs = 1:numvars)
for(i in 1:numvars) {
  column <- train.df[, i]
  NAs <- sum(is.na(column))
  numColNA.df[i, 2] <- NAs
}
numColNA.df
names(train.df)[6]
numobs <- nrow(train.df)
numObNA.df <- tibble(rownum = 1:numobs, NAs = 1:numobs)
for(i in 1:numobs) {
  rows <- train.df[i, ]
  NAs <- sum(is.na(rows))
  numObNA.df[i, 2] <- NAs
}
max(numObNA.df$NAs)
```


It looks like all NA values exist within one column, the age column. Because we must make a prediction for each individual, we will find the average age for individuals for a variety of different age groups as defined by the range of ages for people with certain titles or prefixes to their names.

####Organize data to by name prefix

```{r}

male.df <- train.df %>% filter(Sex == "male")
female.df <- train.df %>% filter(Sex == "female")

maleNames <- male.df$Name
femaleNames <- female.df$Name
dataNames <- train.df$Name

Master.df <- male.df %>%
  filter(str_detect(maleNames, "Master.") == TRUE)
Miss.df <- female.df %>%
  filter(str_detect(femaleNames, "Miss.") == TRUE)
Mr.df <- male.df %>%
  filter(str_detect(maleNames, "Mr.") == TRUE)
Mrs.df <- female.df %>%
  filter(str_detect(femaleNames, "Mrs.") == TRUE)
Dr.df <- train.df %>%
  filter(str_detect(dataNames, "Dr.") == TRUE)

```

####Find average ages, range, and median for potential use later on

```{r}
Master.min <- min(na.omit(Master.df$Age))
Master.max <- max(na.omit(Master.df$Age))
Master.range <- Master.min:Master.max
Master.mean <- mean(Master.df$Age, na.rm = TRUE)
Master.median <- median(Master.df$Age, na.rm = TRUE)

Miss.min <- min(na.omit(Miss.df$Age))
Miss.max <- max(na.omit(Miss.df$Age))
Miss.range <- Miss.min:Miss.max
Miss.mean <- mean(Miss.df$Age, na.rm = TRUE)
Miss.median <- median(Miss.df$Age, na.rm = TRUE)

Mr.min <- min(na.omit(Mr.df$Age))
Mr.max <- max(na.omit(Mr.df$Age))
Mr.range <- Mr.min:Mr.max
Mr.mean <- mean(Mr.df$Age, na.rm = TRUE)
Mr.median <- median(Mr.df$Age, na.rm = TRUE)

Mrs.min <- min(na.omit(Mrs.df$Age))
Mrs.max <- max(na.omit(Mrs.df$Age))
Mrs.range <- Mrs.min:Mrs.max
Mrs.mean <- mean(Mrs.df$Age, na.rm = TRUE)
Mrs.median <- median(Mrs.df$Age, na.rm = TRUE)

Dr.min <- min(na.omit(Dr.df$Age))
Dr.max <- max(na.omit(Dr.df$Age))
Dr.range <- Dr.min:Dr.max
Dr.mean <- mean(Dr.df$Age, na.rm = TRUE)
Dr.median <- median(Dr.df$Age, na.rm = TRUE)
```

####Replace NAs with mean values

```{r}
prefixs <- c("Mr.", "Mrs.", "Miss.", "Master.", "Dr.")
meanvals <- c(Mr.mean, Mrs.mean, Miss.mean, Master.mean, Dr.mean)
numrows <- nrow(train.df)

for(i in 1:5){
  for(a in 1:numrows){
    if(str_detect(train.df[a, 4], prefixs[i])){
      if(is.na(train.df[a, 6])){
        train.df[a, 6] <- meanvals[i]
      }
    }
  }
}
```


###Remove variables that aren't useful for modeling techniques or empty 

There don't appear to be many entries for the "Cabin" variable so we will remove that, as well as "Name", and "Ticket"

```{r}
train3.df <- train.df %>%
  select(-c("Cabin", "Name", "Ticket"))

#consider also removing the passenger ID

```

###Change factor variables to have integer values
```{r}
train3.2.df <- train3.df %>%
  filter(Embarked != "") %>%
  mutate(Embarked = as.character(Embarked)) %>%
  mutate(Embarked = as.factor(Embarked))
#C=1, Q=2, S=3
for(i in 1:3){
  levels(train3.2.df$Embarked)[i] <- i
 }

train3.3.df <- train3.2.df %>%
  mutate(Embarked = as.numeric(Embarked))

#more succinct way of changing column type than mutate is:
#train.df[, Embarked] <- as.factor(train.df$Embarked, c(1,2,3), labels = c(1,2,3))
#other applications of this syntax besides just changing things into factors
```

###Dummy Variable for Sex

```{r}
train4.df <- train3.3.df %>%
  mutate(IsMale = ifelse(Sex == "male", 1, 0)) %>%
  select(-Sex)
train4.df <- as_tibble(train4.df)

#train.df[, Sex] <- as.numeric(Sex)
```

###Repeat all of these steps for the testing data frame

```{r}
male.df <- test.df %>% filter(Sex == "male")
female.df <- test.df %>% filter(Sex == "female")

maleNames <- male.df$Name
femaleNames <- female.df$Name
dataNames <- test.df$Name

Master.df <- male.df %>%
  filter(str_detect(maleNames, "Master.") == TRUE)
Miss.df <- female.df %>%
  filter(str_detect(femaleNames, "Miss.") == TRUE | str_detect(femaleNames, "Ms.") == TRUE)
Mr.df <- male.df %>%
  filter(str_detect(maleNames, "Mr.") == TRUE)
Mrs.df <- female.df %>%
  filter(str_detect(femaleNames, "Mrs.") == TRUE)

#find which titles have NAs beside the typical ones
other.df <- test.df %>%
  filter(is.na(test.df$Age)) 

other.df <- other.df %>%
  filter(str_detect(other.df$Name, "Mr.|Mrs.|Master.|Miss.", negate = TRUE) == TRUE)

Master.min <- min(na.omit(Master.df$Age))
Master.max <- max(na.omit(Master.df$Age))
Master.range <- Master.min:Master.max
Master.mean <- mean(Master.df$Age, na.rm = TRUE)
Master.median <- median(Master.df$Age, na.rm = TRUE)

Miss.min <- min(na.omit(Miss.df$Age))
Miss.max <- max(na.omit(Miss.df$Age))
Miss.range <- Miss.min:Miss.max
Miss.mean <- mean(Miss.df$Age, na.rm = TRUE)
Miss.median <- median(Miss.df$Age, na.rm = TRUE)

Mr.min <- min(na.omit(Mr.df$Age))
Mr.max <- max(na.omit(Mr.df$Age))
Mr.range <- Mr.min:Mr.max
Mr.mean <- mean(Mr.df$Age, na.rm = TRUE)
Mr.median <- median(Mr.df$Age, na.rm = TRUE)

Mrs.min <- min(na.omit(Mrs.df$Age))
Mrs.max <- max(na.omit(Mrs.df$Age))
Mrs.range <- Mrs.min:Mrs.max
Mrs.mean <- mean(Mrs.df$Age, na.rm = TRUE)
Mrs.median <- median(Mrs.df$Age, na.rm = TRUE)

prefixs <- c("Mr.", "Mrs.", "Miss.", "Master.", "Ms.")
meanvals <- c(Mr.mean, Mrs.mean, Miss.mean, Master.mean, Miss.mean)
numrows <- nrow(test.df)

for(i in 1:5){
  for(a in 1:numrows){
    if(str_detect(test.df[a, 3], prefixs[i])){
      if(is.na(test.df[a, 5])){
        test.df[a, 5] <- meanvals[i]
      }
    }
  }
}

test3.df <- test.df %>%
  select(-c("Cabin", "Name", "Ticket"))

test3.2.df <- test3.df %>%
  filter(Embarked != "") %>%
  mutate(Embarked = as.character(Embarked)) %>%
  mutate(Embarked = as.factor(Embarked))

for(i in 1:3){
  levels(test3.2.df$Embarked)[i] <- i
 }

test3.3.df <- test3.2.df %>%
  mutate(Embarked = as.numeric(Embarked))

test4.df <- test3.3.df %>%
  mutate(IsMale = ifelse(Sex == "male", 1, 0)) %>%
  select(-Sex)

#there is one NA in the Fare column which belongs to a 60 year old male so we will find the average fare price for men who are 60

male60.df <- male.df %>%
  filter(Age >= 60)

fare.mean <- mean(male60.df$Fare, na.rm = TRUE)
test4.df[153, 6] <- fare.mean

test4.df <- as_tibble(test4.df)

```

##Exploratory Data Analysis

###Take a Look at the first few observations and variables
```{r}
head(train3.df)
names(train3.df)
```

###Make Some visualizations of the variables

```{r}
ggplot(train3.df)+
  geom_bar(aes(x = Sex))+
  ggtitle("Number of Men and Women Aboard")
```

There are considerably more men aboard the titanic in this data set, so it would be logical to assume that considerably more men are to not have survived.

```{r}
#let's look at the distribution of age for men, women, and all
#let's organize train.df to visualize the above 
train.male.df <- train3.df %>%
  group_by(Age) %>%
  summarise(men = sum(Sex == "male", na.rm = TRUE))
train.female.df <- train3.df %>%
  group_by(Age) %>% 
  summarise(women = sum(Sex == "female", na.rm = TRUE))
train.mf.df <- left_join(train.male.df, train.female.df) %>%
  filter(is.na(Age) == FALSE)

#time to make plots
ggplot(train.mf.df)+
  geom_point(aes(x = Age, y = men, color = "Male"))+
  geom_smooth(aes(x = Age, y = men, color = "Male"), se = FALSE)+
  geom_point(aes(x = Age, y = women, color = "Female"))+
  geom_smooth(aes(x = Age, y = women, color = "Female"), se = FALSE)+
  ylab("Count")+
  ggtitle("Distribution of Men and Women by Age")
  scale_color_manual(name = "Sex",
                     values = c("Male" = "blue", "Female" = "pink"))

```

Seems to be a slightly left skewed normal distribution of men and women. Therefore, a greater number of young individuals will not survive most likely.

##KNN model

###Organize data for knn.cv()

```{r}
input.train <- as.data.frame(train4.df %>% select(-Survived))
output.train <- as.vector(as.matrix(train4.df[,2]))
class.train <- as.factor(output.train)

input.test <- as.data.frame(test4.df)
#output.test <- as.vector(as.matrix(test4.df[,2]))
#class.test <- as.factor(output.test)
```

###Cross-validate to find best K

```{r}
kvals <- 2:20
kvals_tb <- tibble(k = kvals, err = kvals)
for(k in kvals){
  knn.cross_valid <- knn.cv(input.train, class.train, k)
  err <- sum(knn.cross_valid != class.train)/length(class.train)
  kvals_tb[k-1, 2] <- err
}

min_err_ind <- which.min(kvals_tb$err)
best_k <- (kvals_tb$k)[min_err_ind]
```

###Model predictions

```{r}
knn.mod <- knn(input.train, input.test, class.train, k = best_k)
write.csv(knn.mod, "knnPred.csv", row.names = FALSE)
```

##Linear Model

###Get an estimate of accuracy for our model
```{r}
#split the data randomly a number of times and train and test over each split each time
cv.lm(data = train4.df, form.lm = Survived ~ ., m = 10)

#lets do 100 splits
test.results <- c(1:100)

for(i in 1:100){
  n <- nrow(train4.df)
  train.points <- sample(1:n, n/2, replace = FALSE)
  train.points.df <- train4.df[train.points, ]
  test.points.df <- train4.df[-train.points, ]
  test.points.input <- test.points.df[, -2]
  test.points.output <- test.points.df[, 2]

  lm.mod1 <- lm(Survived ~ ., data = train.points.df)
  lm.pred1 <- predict(lm.mod1, newdata = test.points.input)
  resp <- ifelse(lm.pred1 > .5, 1, 0)
  err <- mean(resp != test.points.output)
  test.results[i] <- err
}

mean(test.results)


```

### Actual model

```{r}
mod.lm <- lm(Survived ~ ., train4.df)
lm.pred <- predict(mod.lm, newdata = test4.df)
lm.resp <- ifelse(lm.pred > .5, 1, 0)

write.csv(lm.resp, "lmPred.csv", row.names = FALSE)
```

##logtistic regression

###Do a similar thing as with LM

```{r}
#lets do 100 splits
test.results <- c(1:100)

for(i in 1:100){
  n <- nrow(train4.df)
  train.points <- sample(1:n, n/2, replace = FALSE)
  train.points.df <- train4.df[train.points, ]
  test.points.df <- train4.df[-train.points, ]
  test.points.input <- test.points.df[, -2]
  test.points.output <- test.points.df[, 2]

  glm.mod1 <- glm(Survived ~ ., family = "binomial", data = train.points.df)
  glm.pred1 <- predict(glm.mod1, newdata = test.points.input)
  resp <- ifelse(glm.pred1 > .5, 1, 0)
  err <- mean(resp != test.points.output)
  test.results[i] <- err
}

mean(test.results)
```

###Actual Model

```{r}
glm.mod <- glm(Survived ~ ., family = "binomial", data = train4.df)
anova(glm.mod)
summary(glm.mod)
glm.pred <- predict(glm.mod, newdata = test4.df)
glm.resp <- ifelse(glm.pred > .5, 1, 0)

write.csv(glm.resp, "glmPred.csv", row.names = FALSE)
```


##Random Forest

###Cross-validate and find optimal value for ntrees
####Set up parallel computing for iteration
```{r}
#this is slow so lets use parallel computing
library(parallel)
library(doParallel)
library(foreach)

numcores <- detectCores()
registerDoParallel(numcores)

```

####cross-validation
```{r}
#use foreach and %dopar% for parallel
train5.df <- train4.df %>%
  mutate(Survived = as.factor(Survived))

train.points.input <- as.data.frame(train5.df[, -2])
train.points.output <- as.factor(as.vector(as.matrix(train5.df[, 2])))

ntrees <- 2:200

rf.err <- as_tibble(
  foreach(i = ntrees, .combine = rbind, .packages = c('randomForest', 'rfUtilities')) %dopar% {
  rf.mod <- randomForest(train.points.input, train.points.output, ntree = i)
  rf.cross <- rf.crossValidation(rf.mod, train.points.input)
  c(mean(rf.cross$cross.validation$cv.oob$OOB),
    mean(rf.cross$cross.validation$cv.oob$kappa))
  })

names(rf.err) <- c("err", "kappa")

stopImplicitCluster()

#for understanding user, producer, and oob accuracies, and Kappa value
# http://gsp.humboldt.edu/olm_2015/Courses/GSP_216_Online/lesson6-2/metrics.html

rf.cv.err <- tibble(ntree = ntrees, error = rf.err)

#OOB error is the out-of-bag error or simply, the error rate for the cross-validation
```


```{r}
#without parallel computing
#use foreach and %dopar% for parallel
train5.df <- train4.df %>%
  mutate(Survived = as.factor(Survived))

train.points.input <- as.data.frame(train5.df[, -2])
train.points.output <- as.factor(as.vector(as.matrix(train5.df[, 2])))

ntrees <- 2:100

rf.cv.err <- tibble(ntree = ntrees, error = ntrees)

for(t in ntrees){
  rf.mod <- randomForest(train.points.input, train.points.output, ntree = t)
  rf.cross <- rf.crossValidation(rf.mod, train.points.input)
  err <- mean(rf.cross$cross.validation$cv.oob$OOB) 
  rf.cv.err[t-1, 2] <- err
}

```


####min error n tree

###random forest model using optimal ntree


##Compare the predictions made

```{r}
all_preds <- cbind(knn = as.numeric(knn.mod)-1, lm = lm.resp, glm = glm.resp)
View(all_preds)
```















```{r}
ncols <- ncol(input.test)
for(i in 1:ncols){
  print(sum(is.na(input.test[, i])))
}

names(input.test)[6]
which(is.na(input.test$Fare))
```

```{r}
foreach (i=1:3, .combine=rbind) %dopar% {
  c(sqrt(i), i^2)
}
```

